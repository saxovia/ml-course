{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6375bed3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "719ee39ebd4d64f35635255621551af7",
     "grade": false,
     "grade_id": "cell-50170e1c92e960be",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Assignment 5\n",
    "(v.2.1)\n",
    "## Lecture 9 & 10 - Reinforcement Learning and Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccef66fa-9b16-458d-9f4e-ae52af9aa634",
   "metadata": {
    "tags": []
   },
   "source": [
    "# General Instructions\n",
    "- Jupyter notebook is supposed to be run cell by cell in order, please do not skip any code cell, this will cause some errors. Also running cells back and forth sometimes might also incur errors. If you feel you lost your track, you can click \"Kernel->Restart\" from the menu to restart the process.\n",
    "- Before submitting your assignment, ensure that it does not contain trivial errors by pressing the \"validate\" button at the top.\n",
    "- Your implementations are supposed to be added to the places where it reads \"YOUR CODE HERE\". Please also remove the \"raise NotImplementedError()\" line before submitting.\n",
    "- Please DO NOT change the metadata of any cell, cells for demo and instructions are not editable.\n",
    "- Please DO NOT change the order of solution cell and test cell, you will lost points if the order is changed.\n",
    "- You can copy lines of code from cells that are not editable, but please DO NOT copy and paste them as cells, this may incur validation error. \n",
    "- You can add extra cells or code to help double-check your solution, but please make sure that variables required by tasks are not overwritten, or just delete those extra cells before submitting.\n",
    "- Please DO NOT change file names in you submission, renamed files can not be recognized by the grading system.\n",
    "- Reading the documentation of Python libraries is always a good practice, all the Python libraries (Numpy, Pandas, Sklearn,etc.) we utilized in this course provide very well organized documentation for each method/class/function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a0bb5c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a3215b4b17a84d8e21575db3c4a32757",
     "grade": false,
     "grade_id": "cell-f87255cf46066085",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "### Learning Goals\n",
    "After successfully completing this assignment, you should be able to:\n",
    "- explain why exploration and exploitation are important concepts in reinforcement learning problems\n",
    "- understand better the Multi-Armed Bandit problem\n",
    "- implement a strategy for maximizing the cumulative reward (UCB)\n",
    "- understand the Frozen Lake game/problem\n",
    "- implement an algorithm for training an agent to find an optimal path through this grid game (Q-learning)\n",
    "- understand on an intuitive level how different parameters impacts the performance\n",
    "\n",
    "## Table of Contents\n",
    "1. [Multi-Armed Bandit](#multi_armed_bandit)\n",
    "2. [Frozen Lake Game](#frozen_lake_game)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913cb649",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6c6f1eb509d0dff5474e52b11879523e",
     "grade": false,
     "grade_id": "cell-e2d620d526c5f3d8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Multi-Armed Bandit <a class=\"anchor\" id=\"multi_armed_bandit\"></a>\n",
    "\n",
    "![Multi-armed bandit](https://leovan.me/images/cn/2020-05-16-multi-armed-bandit/compulsive-gambling.png \"Multi-armed bandit\")\n",
    "\n",
    "Here we will focus on the \"Multi-armed bandit\" problem. There are 10 one-armed bandit machines where each have different probabilities for winning and with different reward ranges. As we have a limited amount of coins to spend, the goal is to play on these machines in a way that maximizes the cumulative reward. An action at every time step involves selecting and playing on  one of these ten one-armed bandits, or simply arms. For each action, i.e. for each arm that we play, we observe the reward which is a value between 0 and 10 (€). Since we know nothing about the reward properties of these machines to start with, we need to implement a suitable strategy that is based around observing the rewards that we get from playing on these machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5ea70aa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c013b7e034ade9953085ff274ad73c57",
     "grade": false,
     "grade_id": "cell-0f2103e7721e7d98",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the libraries we will be using\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "random.seed(1234)\n",
    "import numpy as np\n",
    "import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051d4b13",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "45f496ce7013f8fcd9386249452c2149",
     "grade": false,
     "grade_id": "cell-f735c01ae7155155",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Dataset\n",
    "The dataset that we will use here is `10_armed_bandit_data.csv`. Each row in this dataset shows the reward of playing on each arm at a given time step. With this way of representing the data, the reward from each arm changes for each time step, meaning that also the order in which the arms are selected will influence the outcome. Since there are 1000 rows in the dataset, it means we have 1000 coins to play with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "116f9956",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fdb68f86b07cc5e5cbb27612cf9f68ab",
     "grade": false,
     "grade_id": "cell-bc85ecd8866b57b8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Arm 0  Arm 1  Arm 2  Arm 3  Arm 4  Arm 5  Arm 6  Arm 7  Arm 8  Arm 9\n",
      "0      0      0      0      4      2      0      0      0      0      1\n",
      "1      0      0      6      0      0      0      0      0      0      0\n",
      "2      0      0      0      0      3      0      0      0      0      0\n",
      "3      0      0      7      0      2      0      1      5      0      0\n",
      "4      0      2      0      5      0      0      0      0      0      0\n"
     ]
    }
   ],
   "source": [
    "# Importing the dataset\n",
    "df = pd.read_csv('10_armed_bandit_data.csv')\n",
    "\n",
    "# Print the first 5 rows from the dataset.\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a195f67c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2b4dd1047abc8501342b5fa30e4baa07",
     "grade": false,
     "grade_id": "cell-6dafc102f4280ff6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Random Selection\n",
    "In this first part we adopt a simple strategy that involves naively selecting and playing on a random arm each time, independent of the rewards we get."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eac19a7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "efa7f93ff2e82292d7de3dc148d11771",
     "grade": false,
     "grade_id": "cell-e939a12af6e0a84d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-warning\">\n",
    "    \n",
    "## Student Task A5.1\n",
    "\n",
    "Your task is to implement the Random Selection strategy - selecting a random arm (or action) $a$ at every time step $t$ - by completing the `random_selection()` function. Our budget is 1000 coins.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "72d79c53",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9b7cc16868552d72645527bc63e2dc3b",
     "grade": false,
     "grade_id": "cell-545908122b24ea7e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward using Random Selection: 754\n",
      "\n",
      "  Arm    Times selected\n",
      "-----  ----------------\n",
      "    9               120\n",
      "    8               111\n",
      "    6               107\n",
      "    2               104\n",
      "    7                99\n",
      "    5                95\n",
      "    0                94\n",
      "    1                91\n",
      "    4                90\n",
      "    3                89\n"
     ]
    }
   ],
   "source": [
    "# Implementing Random Selection \n",
    "def random_selection(data):\n",
    "    \n",
    "    # There are 10 columns in the dataset, one for each arm/machine.\n",
    "    # The row count represents the number of times we play, i.e., how many coins we will spend. \n",
    "    coins, arms = data.shape  # (1000, 10)\n",
    "    \n",
    "    reward_sum_per_arm = [0] * arms\n",
    "    selection_count_per_arm = [0] * arms\n",
    "    \n",
    "    for t in range(0, coins):\n",
    "        \n",
    "        ## Select a random arm (action) among the 10 available arms\n",
    "        # a = random. ...\n",
    "        \n",
    "        ## Fetch the reward for selecting the given arm (a) at the given time step (t) from the data\n",
    "        # reward = data.values[...]\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        #raise NotImplementedError()\n",
    "        a = random.randint(0, arms - 1)\n",
    "        reward = data.values[t,a]\n",
    "\n",
    "        selection_count_per_arm[a] += 1\n",
    "        reward_sum_per_arm[a] += reward\n",
    "\n",
    "    return reward_sum_per_arm, selection_count_per_arm\n",
    "\n",
    "\n",
    "# Run the selection algorithm\n",
    "reward_sum_per_arm, selection_count_per_arm = random_selection(df)\n",
    "\n",
    "# Print the outcome and some statistics\n",
    "print('Total reward using Random Selection: {}'.format(sum(reward_sum_per_arm)))\n",
    "\n",
    "sorted_indxs = np.argsort([-x for x in selection_count_per_arm])\n",
    "tab_data = [[indx, selection_count_per_arm[indx]] for indx in sorted_indxs]\n",
    "headers = ['Arm', 'Times selected']\n",
    "print('\\n' + tabulate.tabulate(tab_data, headers=headers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "552d33e8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "40b787b351f266bfaa444effae228b6a",
     "grade": true,
     "grade_id": "cell-0584325e68b31952",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this cell is for tests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c3e98e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "28c9ca4daf268b4858a369118422f3d5",
     "grade": false,
     "grade_id": "cell-ac3299f40a62e838",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Upper Confidence Bound (UCB) Selection\n",
    "\n",
    "Random selection of arms to play on is probably not the optimal approach. To improve our reward we want to use the UCB algorithm to calculate $a_t$ which is the action, or arm, at time $t$ with maximum UCB:\n",
    "$$a_t = \\underset{a\\in{A}}{\\operatorname{argmax}}\\biggr[Q_t(a) + c \\sqrt{\\frac{\\log{t}}{N_t(a)}}\\biggr]$$\n",
    "<br>\n",
    "Where $Q_t(a)$ is the estimated value of action (arm) $a$ at time step $t$:\n",
    "$$Q_t(a) = \\frac{ \\sum_{i=1}^{t-1} R_{i}\\mathbf{1}_{A_i = a} }{ \\sum_{i=1}^{t-1} \\mathbf{1}_{A_i = a} }$$\n",
    "<br>\n",
    "$t$ - current time step.<br>\n",
    "$N_t(a)$ - the number of times that action $a$ has been selected, prior to time $t$.<br>\n",
    "$c$ - confidence value that controls the level of *exploration* vs *exploitation*. Higher $c$ means higher exploration rate (... of arms that have not been explored much or yielded much reward so far).<br>\n",
    "$\\sum_{i=1}^{t-1} R_{i}\\mathbf{1}_{A_i = a}$ - sum of rewards from selecting $a$ up to $t$-1<br>\n",
    "$\\sum_{i=1}^{t-1} \\mathbf{1}_{A_i = a}$ - number of times $a$ has been selected up to $t$-1<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78244e7e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3bf0dd1d41b5a236b0eb722fbbc55bbf",
     "grade": false,
     "grade_id": "cell-3acd4c88a65b8862",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-warning\">\n",
    "    \n",
    "## Student Task A5.2\n",
    "\n",
    "In this assignment your task is to implement the above UCB algorithm, `max_ucb_action()`. As inputs our function takes $c$, $t$, a list storing the accumulated rewards per arm (`reward_sum_per_arm`), and a list with informaion about how many times each arm has been selected so far (`selection_count_per_arm`, this is $\\{N_t(A)\\}$).\n",
    "<br><br>\n",
    "Note, in the beginning the value of $N_t(a)$ will be zero. To avoid the division by zero problem, we can, in those cases, set it to a number that is close to zero, e.g., 1e-100.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "483be6fe",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e0febec5652f2fbcbd6396fc6a585a52",
     "grade": false,
     "grade_id": "cell-a7d2a05e1457eb03",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example showing how the input arguments might look like at t = 40.\n",
    "# c = 1\n",
    "# t = 40\n",
    "# reward_sum_per_arm = [0, 12, 0, 2, 1, 0, 0, 0, 0, 0]\n",
    "# selection_count_per_arm = [3, 15, 3, 5, 4, 3, 2, 2, 2, 2]\n",
    "\n",
    "# reward_sum_per_arm and selection_count_per_arm stores the cummulated values up to the current time step t\n",
    "\n",
    "def max_ucb_action(c, t, reward_sum_per_arm, selection_count_per_arm):\n",
    "    arms = len(reward_sum_per_arm)\n",
    "    confidence_per_arm = [0] * arms\n",
    "    \n",
    "    # In this for-loop we calculate the confidence_per_arm (i.e., action a) at the given time step t\n",
    "    for a in range(0, arms):\n",
    "        \n",
    "        Nt = selection_count_per_arm[a]\n",
    "        if Nt == 0:\n",
    "            Nt = 1e-100  # To avoid division by zero error\n",
    "        \n",
    "        # Qt = ...\n",
    "        \n",
    "        ## NB!: use math.log(t+1) to avoid error with math.log(0)\n",
    "        # Ut = ...  \n",
    "        \n",
    "        # confidence_per_arm[a] = ...\n",
    "        \n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        #raise NotImplementedError()\n",
    "        Qt = reward_sum_per_arm[a] / Nt\n",
    "        Ut = c * math.sqrt(math.log(t+1) / Nt)\n",
    "        confidence_per_arm[a] = Qt + Ut\n",
    "        \n",
    "\n",
    "    # Pick the arm/action with the highest upper bound\n",
    "    max_upper_bound_arm = confidence_per_arm.index(max(confidence_per_arm))\n",
    "    return max_upper_bound_arm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "430fc9d7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e2525694eaf205a8dd36ca166afe99b7",
     "grade": true,
     "grade_id": "cell-05d969ea25aaf599",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this cell is for tests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22f2491",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "55e32b29d0dd5e7361780efffeee6131",
     "grade": false,
     "grade_id": "cell-f4c93de7e4fba678",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-warning\">\n",
    "    \n",
    "## Student Task A5.3\n",
    "\n",
    "Now that we have implemented `max_ucb_action()`, the next thing to do is to implement the function `ucb_selection()` where we use the UCB selection approach for playing on these 10 one-armed bandits. Our budget is again 1000 coins.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "63a328bd",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6c982797f18d7dcd699abbb1f6010e7a",
     "grade": false,
     "grade_id": "cell-f969fac19bedee18",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward using UCB Selection: 1200\n",
      "\n",
      "  Arm    Times selected\n",
      "-----  ----------------\n",
      "    3               874\n",
      "    9                61\n",
      "    7                35\n",
      "    4                 6\n",
      "    0                 4\n",
      "    1                 4\n",
      "    2                 4\n",
      "    5                 4\n",
      "    6                 4\n",
      "    8                 4\n"
     ]
    }
   ],
   "source": [
    "# Implementing UCB Selection\n",
    "def ucb_selection(data, c):\n",
    "    \n",
    "    ## Implement this in the same way as the above random_selection() function (from A5.1).\n",
    "    ## The only exception is that you should now use use max_ucb_action() to select arms \n",
    "    ## instead of selecting arms at random.\n",
    "    # ...\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "    coins, arms = data.shape  # (1000, 10)\n",
    "    reward_sum_per_arm = [0] * arms\n",
    "    selection_count_per_arm = [0] * arms\n",
    "    #return reward_sum_per_arm, selection_count_per_arm\n",
    "\n",
    "\n",
    "    \n",
    "    # There are 10 columns in the dataset, one for each arm/machine.\n",
    "    # The row count represents the number of times we play, i.e., how many coins we will spend. \n",
    "    coins, arms = data.shape  # (1000, 10)\n",
    "    \n",
    "    reward_sum_per_arm = [0] * arms\n",
    "    selection_count_per_arm = [0] * arms\n",
    "    \n",
    "    for t in range(0, coins):\n",
    "        \n",
    "        ## Select a random arm (action) among the 10 available arms\n",
    "        # a = random. ...\n",
    "        \n",
    "        ## Fetch the reward for selecting the given arm (a) at the given time step (t) from the data\n",
    "        # reward = data.values[...]\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        #raise NotImplementedError()\n",
    "        a = max_ucb_action(c, t, reward_sum_per_arm, selection_count_per_arm)\n",
    "        reward = data.values[t,a]\n",
    "        \n",
    "        selection_count_per_arm[a] += 1\n",
    "        reward_sum_per_arm[a] += reward\n",
    "\n",
    "    return reward_sum_per_arm, selection_count_per_arm\n",
    "    \n",
    "\n",
    "# Run the selection algorithm (by default we have set c = 1)\n",
    "c = 1\n",
    "reward_sum_per_arm, selection_count_per_arm = ucb_selection(df, c)\n",
    "\n",
    "# Sanity check\n",
    "assert(sum(ucb_selection(df, c=1)[0]) == 1200)\n",
    "\n",
    "\n",
    "# Print the outcome and some statistics\n",
    "print('Total reward using UCB Selection:', sum(reward_sum_per_arm))\n",
    "\n",
    "sorted_indxs = np.argsort([-x for x in selection_count_per_arm])\n",
    "tab_data = [[indx, selection_count_per_arm[indx]] for indx in sorted_indxs]\n",
    "headers = ['Arm', 'Times selected']\n",
    "print('\\n' + tabulate.tabulate(tab_data, headers=headers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "19ab6bbf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ce91f3833158777b7329100161aa3143",
     "grade": true,
     "grade_id": "cell-4b9cc6f50f975e03",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this cell is for tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a75c177",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "268e0b9a23fda9fd2c7e1f56b9265130",
     "grade": false,
     "grade_id": "cell-71dfe6eb8e814916",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e7807c3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3f7464b5f5250c27a2ad01d9ba7d2ccf",
     "grade": false,
     "grade_id": "cell-a01a7f187b223318",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# The Frozen Lake Game <a class=\"anchor\" id=\"frozen_lake_game\"></a>\n",
    "<img src=\"https://debuggirl.files.wordpress.com/2013/03/img_43081.jpg\" alt=\"drawing\" width=\"800\"/>\n",
    "\n",
    "The Frozen Lake game is about safely navigating a frozen lake whose ice happen have several hidden cracks or holes in it, stepping on any of these will result in you falling into the water. The goal is to reach a part of the lake where a hidden treasure is located. Both the weak spots (holes) and the goal are not known beforehand by the player. Here we will train an *agent* to navigate this lake safely and reach the goal. To do so we will be using the **Q-learning** algorithm. Here we use the **Q-value function** $Q({s, a}) = v$ that takes as input the current state $s$ and possible action $a$, and returns its value $v$. The frozen lake is represented as a grid of cells (gridworld), where each cell is associated with a state that can be start `S`, frozen lake `F`, hole in the ice `H`, or goal `G`, and the actions available at each state is to navigate *left*, *down*, *right*, or *up*.<br>\n",
    "<br>\n",
    "When playing this game the agent will be iteratively doing the following:\n",
    "1. Choose an action given the current state of the agent (calculate where to move).\n",
    "2. Perform the chosen action (move to the chosen cell).\n",
    "3. Upadete the Q-value function (the Q-table) based on the reward from performing this action.\n",
    "4. Set the new state of the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7e90a5eb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "28e44a0a9e6ecda76e2148844c9d54c6",
     "grade": false,
     "grade_id": "cell-e03448fef9ecb599",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21.0\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "# Import the libraries we will be using\n",
    "import gym  # gym-0.21.0\n",
    "#gymnasium as gym\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "\n",
    "print(gym.__version__)\n",
    "\n",
    "# Initiate the FrozenLake game\n",
    "env = gym.make('FrozenLake-v1', is_slippery=False, map_name=\"4x4\")\n",
    "\n",
    "# Next we render the game board\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9941c82c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "caf5abd8e48cb5d3b13cde7e740be532",
     "grade": false,
     "grade_id": "cell-54b0db6bd3f5b3a2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "After running the above, you should should see the gameboard, i.e. the frozen lake.\n",
    "<br>\n",
    "SFFF</br>\n",
    "FHFH<br>\n",
    "FFFH<br>\n",
    "HFFG<br>\n",
    "\n",
    "\n",
    "It consist of the following cells:\n",
    " - S - Start.\n",
    " - F - Frozen lake, which is safe to walk on.\n",
    " - H - Hole in the ice. Reaching this cell ends the round.\n",
    " - G - Goal. Reaching this cell gives +1 point and ends the round.\n",
    "\n",
    "As mentioned, we will now train an agent to navigate this frozen lake and ultimately reach the goal (`G` cell) with as few steps/turns as possible without falling into the lake (stepping on any of the `H` cells). At start the agent does not know where the goal is or what cells are safe to walk on. However, with repeated trials, combined with the delayed feedback reward gained from stepping on the various cells, the agent should eventually learn to navigate this lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a11f6b3f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "10091a7208372d6eb4366a9a116e46d0",
     "grade": false,
     "grade_id": "cell-c1ed9720d13e6350",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The frozen lake (gameboard) consist of 16 states (cells) and 4 actions for each of these.\n",
      "So for each state there are four actions availble:\n",
      "0 = ◀️ left\n",
      "1 = 🔽 down\n",
      "2 = ▶️ right\n",
      "3 = 🔼 up\n"
     ]
    }
   ],
   "source": [
    "state_space_size = env.observation_space.n\n",
    "action_space_size = env.action_space.n\n",
    "print('The frozen lake (gameboard) consist of {} states (cells) and {} actions for each of these.'.format(state_space_size, action_space_size))\n",
    "print('So for each state there are four actions availble:\\n0 = ◀️ left\\n1 = 🔽 down\\n2 = ▶️ right\\n3 = 🔼 up')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f734a667",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d4fa28d623a25f276ab8d18a5431d261",
     "grade": false,
     "grade_id": "cell-dba9577676343545",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q-Table\n",
    "We will be using a **Q-table** as the underlying representation of the value function (Q-value function). For this type of game board, the Q-table is a well suited representation to use. This table will store all the knowledge gained by the agent - each *state* will have information about the *value* of taking any of the available *actions* (◀️, 🔽, ▶️, 🔼) when the policy is to find the hidden treasure without falling into the lake. To start with, all the q-values in this table will be zero, but these will be updated as the agent is playing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b848f4b1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "859dd1ecd1d260fd445ff9d07bbd477d",
     "grade": false,
     "grade_id": "cell-3905c01e39ae4d2f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-table:\n",
      "[0. 0. 0. 0.]\t<-- state [0,0]\n",
      "[0. 0. 0. 0.]\t<-- state [0,1]\n",
      "[0. 0. 0. 0.]\t<-- state [0,2]\n",
      "[0. 0. 0. 0.]\t<-- state [0,3]\n",
      "[0. 0. 0. 0.]\t<-- state [1,0]\n",
      "[0. 0. 0. 0.]\t<-- state [1,1]\n",
      "[0. 0. 0. 0.]\t<-- state [1,2]\n",
      "[0. 0. 0. 0.]\t<-- state [1,3]\n",
      "[0. 0. 0. 0.]\t<-- state [2,0]\n",
      "[0. 0. 0. 0.]\t<-- state [2,1]\n",
      "[0. 0. 0. 0.]\t<-- state [2,2]\n",
      "[0. 0. 0. 0.]\t<-- state [2,3]\n",
      "[0. 0. 0. 0.]\t<-- state [3,0]\n",
      "[0. 0. 0. 0.]\t<-- state [3,1]\n",
      "[0. 0. 0. 0.]\t<-- state [3,2]\n",
      "[0. 0. 0. 0.]\t<-- state [3,3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q_table = np.zeros((state_space_size, action_space_size))\n",
    "\n",
    "# Print some info about it\n",
    "s = 'Q-table:\\n'\n",
    "i = 0\n",
    "j = 0\n",
    "for cr in q_table:\n",
    "    s += '{}\\t<-- state [{},{}]\\n'.format(cr, i, j)\n",
    "    j += 1\n",
    "    if j >= action_space_size:\n",
    "        j = 0\n",
    "        i += 1\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae01632",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "45523bca5f0b1eadb4d0ae0e11a711f4",
     "grade": false,
     "grade_id": "cell-44427ed4e698c56b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "With an empty Q-table the agent do not know anything about what actions are better than others. In other words, the Q-value function will always return zero for every possible action at each state. Thus in the beginning the agent will instead be taking random actions while exploring the board. This is called *exploration*. However, after having gained some knowledge about the board, the agent can start relying on the experience gained and reflected in the Q-value function. This is called *exploitation*. We will be using the *epsilon greedy strategy* to find a balance between *exploration* (random actions) and *exploitation* (optional actions according to the Q-value function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b62e5b02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Here we introduce some parameters to use in the Q-training algorithm.\n",
    "\n",
    "# Learning rate (value between 0 and 1)\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Reward discount used in the Bellman equation (value between 0 and 1)\n",
    "discount_factor = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf44db8b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c26a5a9be9dd233b4777541d64a65efb",
     "grade": false,
     "grade_id": "cell-96a040c9a8a6c9c6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## The Bellman Equation\n",
    "For updating a state in the Q-table we will here be using the following version of the *Bellman equation*:<br>\n",
    "$$Q^{updated}(s_{t}, a_{t}) := (1-\\alpha)Q(s_{t}, a_{t}) + \\alpha (r_{t} + \\gamma \\underset{a}{\\max}Q(s_{t+1},a))$$\n",
    "$s_{t}$ - current state.<br>\n",
    "$a_{t}$ - chosen action.<br>\n",
    "$\\alpha$ - learning rate.<br>\n",
    "$r_{t}$ - reward from taking the action $a_{t}$.<br>\n",
    "$\\gamma$ - discount factor.<br>\n",
    "$s_{t+1}$ - new state after taking action $a_{t}$.<br>\n",
    "$\\underset{a}{\\max}Q(s_{t+1},a)$ - the optional value possible from $s_{t+1}$.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80054275",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8f283074330a952f70418d59c8435d89",
     "grade": false,
     "grade_id": "cell-da18419fc78cb4a7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-warning\">\n",
    "    \n",
    "## Student Task A5.4\n",
    "\n",
    "<strong><i><u>Your task<i></u></strong> is to implement the Bellman equation in `q_update()` that calculates and returns the new value for $Q^{updated}(s_{t}, a_{t})$.<br>It takes as input: Q-table, $s_{t}$, $a_{t}$, $s_{t+1}$, $r_{t}$, $\\alpha$, and $\\gamma$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e1915777",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1c61914b7a0d78a93e44fea476eb3a3b",
     "grade": false,
     "grade_id": "cell-4b8c88159a873820",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def q_update(q_table, state, action, new_state, reward, learning_rate, discount_factor):\n",
    "    # Calculate the new value in the q_table using Bellman equation\n",
    "\n",
    "    ## Implement the Bellman equation as shown above\n",
    "    # q_new = q_table[state, action] * ...\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "    q_new = q_table[state, action] * (1-learning_rate) + learning_rate * ( reward + discount_factor * max(q_table[new_state]))\n",
    "    \n",
    "    return q_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7dc8915d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "13269b7b9f6057b8f3d1b152ccd11711",
     "grade": true,
     "grade_id": "cell-789d006652c57c07",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this cell is for tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "374604c2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1ee27a67fd6751b3b00b6f793eebfccb",
     "grade": false,
     "grade_id": "cell-13c14052c56b4d4e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For the mentioned epsilon greedy strategy we will need a few more parameters.\n",
    "\n",
    "# Upper bound for the exploration rate (value between 0 and 1)\n",
    "max_exploration_rate = 1.0\n",
    "\n",
    "# Lower bound for the exploration rate (value between 0 and 1)\n",
    "min_exploration_rate = 0.01\n",
    "\n",
    "# How much the exploration rate decays per episode (down to the min_exploration_rate, value between 0 and 1).\n",
    "exploration_decay_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cd219d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a4b4f57d614e988f765241ceff72c650",
     "grade": false,
     "grade_id": "cell-6f94d61e0fd38afb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Epsilon Greedy Strategy\n",
    "By using the *epsilon greedy strategy*, we will let the exploration rate start high (1), and then decrease for every episode with exponential decay. The goal here is to find the optimal balance between exploration and exploitation. Each round will consist of 10000 episodes. In the first episodes we want to have a high probility of performing exploration actions to avoid getting stuck in sub-optimal paths (local maximums). However, we want to gradually start leaning more towards exploitation towards the later episodes.<br>\n",
    "<br>\n",
    "We will be using the following formula for calculating the exploration rate for a given episode $\\epsilon(i)$:<br>\n",
    "$$\\epsilon(i) = \\epsilon_{min} + (\\epsilon_{max} - \\epsilon_{min}) \\cdot e^{-\\zeta \\cdot i}$$\n",
    "$\\epsilon$ - exploration rate.<br>\n",
    "$i$ - episode number.<br>\n",
    "$\\epsilon_{min}$ - min exploration rate.<br>\n",
    "$\\epsilon_{max}$ - max exploration rate.<br>\n",
    "$\\zeta$ - exploration decay rate.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dcc3f5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "735a5ac9011cb8937f84d80c81a57150",
     "grade": false,
     "grade_id": "cell-dd5341b2e6ce6da4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-warning\">\n",
    "    \n",
    "## Student Task A5.5\n",
    "\n",
    "Implement the `calc_exploration_rate()` function which calculates and returns the exploration rate using the above formula.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "210d77f4",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5da854273b10ed88aa1c94ffeeb70fa4",
     "grade": false,
     "grade_id": "cell-2d10be506d65f993",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function that calculate the exploration rate for a given episode\n",
    "def calc_exploration_rate(episode, min_exploration_rate, max_exploration_rate, exploration_decay_rate):\n",
    "    ## Calculate the exploration rate\n",
    "    # exploration_rate = ...\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "    exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate) *  math.exp((-exploration_decay_rate * episode))\n",
    "    \n",
    "    return exploration_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "df34e84c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b24c889a17e7c9e8bcb60cb761982dc3",
     "grade": true,
     "grade_id": "cell-307f265b7d244b66",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this cell is for tests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbb551b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0202f2067e61f6852048b4aa025a17c1",
     "grade": false,
     "grade_id": "cell-5007d85f0ea8352f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-warning\">\n",
    "    \n",
    "## Student Task A5.6\n",
    "\n",
    "Next we will make the function for choosing an action given a state. This function includes an *exploration rate threshold* which is a randomly drawn number between 0 and 1. For every action, this thershold makes the agent perform an exploration action with a probability equal to the given *exploration rate*.\n",
    "<br>\n",
    "<strong><i><u>Your task<i></u></strong> is to complete the below function `choose_action()`. This function takes as input the Q-table, current state $s_{t}$, and exploration rate $\\epsilon$. It should return an integer reflecting the chosen action (from the set {0, 1, 2, 3}).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3f932326",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "726c8bed330d6ff38722c534f6b77429",
     "grade": false,
     "grade_id": "cell-188c8ae072cecd71",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def choose_action(q_table, state, exploration_rate):\n",
    "    \n",
    "    ## 1. Pick a random number between 0 and 1\n",
    "    # exploration_rate_threshold = ...\n",
    "    \n",
    "    exploration_rate_threshold = np.random.uniform(0, 1)\n",
    "    ## 2. Exploration vs exploitation\n",
    "    if exploration_rate > exploration_rate_threshold or sum(q_table[state, :]) == 0:\n",
    "        ## Exploration, i.e., random action \n",
    "        # action = ...\n",
    "        #pass\n",
    "        action = np.random.randint(q_table.shape[1])\n",
    "    else:\n",
    "        ## Exploitation, i.e., perform the optimal action at this state according to the Q-table\n",
    "        # action = ...\n",
    "        #pass\n",
    "        action = np.argmax(q_table[state, :])\n",
    "    \n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5411b02b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "95e02f0c02fb5e0d736da3841bd0cfe7",
     "grade": true,
     "grade_id": "cell-c5bab8432aab8bc7",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this cell is for tests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e80ede",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0ce0573824eb87ac08f5dd04fc0e7204",
     "grade": false,
     "grade_id": "cell-8de9391ad7cfd1d9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q-Learning Algorithm\n",
    "Now it is time to train our agent using the Q-learning algorithm as shown below.\n",
    "If everything works as intended, the below code should print some statistics from the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "37d85fe0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of episodes to train\n",
    "num_episodes = 10000\n",
    "\n",
    "# Max number of steps before ending the round\n",
    "max_steps_per_episode = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0803db2f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b95b129b5f85067a736a22b68ee1e637",
     "grade": false,
     "grade_id": "cell-4c4b9b72e5654f6f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Q-learning function\n",
    "def train_q_learn(env, q_table):\n",
    "    \n",
    "    # Q-Learning algorithm\n",
    "    rewards_all_episodes = []\n",
    "    steps_all_episodes = []\n",
    "    exploration_rate = max_exploration_rate\n",
    "\n",
    "    # Play the game for n episodes\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        rewards_current_episode = 0\n",
    "        steps_current_episode = 0\n",
    "\n",
    "        for step in range(max_steps_per_episode):\n",
    "            # Chooce an action. Exploration vs exploitation trade-off with the epsilon greedy strategy\n",
    "            action = choose_action(q_table, state, exploration_rate)\n",
    "\n",
    "            # Perform the chosen action\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "\n",
    "            # Upadete the Q-table\n",
    "            q_table[state, action] = q_update(q_table, state, action, new_state, reward, learning_rate, discount_factor)\n",
    "\n",
    "            # Set the new state\n",
    "            state = new_state\n",
    "\n",
    "            # Collect some statistics\n",
    "            rewards_current_episode += reward\n",
    "            steps_current_episode += 1\n",
    "\n",
    "            if done == True:\n",
    "                break\n",
    "\n",
    "        # Update the exploration rate\n",
    "        exploration_rate = calc_exploration_rate(episode, min_exploration_rate, max_exploration_rate, exploration_decay_rate)\n",
    "\n",
    "        rewards_all_episodes.append(rewards_current_episode)\n",
    "        steps_all_episodes.append(steps_current_episode)\n",
    "\n",
    "\n",
    "    # Print the average rewards\n",
    "    rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes), num_episodes/1000)\n",
    "    steps_per_thousand_episodes = np.split(np.array(steps_all_episodes), num_episodes/1000)\n",
    "\n",
    "    print('Training results, average reward and steps per thousand episodes:')\n",
    "    count = 1000\n",
    "    for i, _ in enumerate(rewards_per_thousand_episodes):\n",
    "        r = rewards_per_thousand_episodes[i]\n",
    "        s = steps_per_thousand_episodes[i]\n",
    "        print('{}: rewards {:.4}, steps {:.4}'.format(count, sum(r/1000), sum(s/1000)))\n",
    "        count += 1000\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "577d21b7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e17ee730c45d98b8e4a78f9dc23c451c",
     "grade": false,
     "grade_id": "cell-d9c3f2482257ac56",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training results, average reward and steps per thousand episodes:\n",
      "1000: rewards 0.277, steps 8.179\n",
      "2000: rewards 0.73, steps 6.839\n",
      "3000: rewards 0.895, steps 6.327\n",
      "4000: rewards 0.969, steps 6.181\n",
      "5000: rewards 0.985, steps 6.089\n",
      "6000: rewards 0.99, steps 6.05\n",
      "7000: rewards 0.982, steps 6.009\n",
      "8000: rewards 0.989, steps 6.029\n",
      "9000: rewards 0.991, steps 6.042\n",
      "10000: rewards 0.987, steps 6.04\n"
     ]
    }
   ],
   "source": [
    "# Run the train_q_learn function\n",
    "q_table = train_q_learn(env, q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7344263d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ec7927a009af2e8e7d5776e0de59ddd9",
     "grade": false,
     "grade_id": "cell-48dc2140c1871f01",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Q-table:\n",
      "[[0.94148015 0.93206535 0.95099005 0.94148015]\n",
      " [0.94148015 0.         0.96059601 0.95099005]\n",
      " [0.95099005 0.970299   0.95099003 0.96059601]\n",
      " [0.96059601 0.         0.87529013 0.82383882]\n",
      " [0.91821032 0.78112473 0.         0.94148015]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.9801     0.         0.96059601]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.19309012 0.         0.94016893 0.57099703]\n",
      " [0.69907888 0.89308457 0.98009987 0.        ]\n",
      " [0.97029643 0.99       0.         0.970299  ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.68000273 0.98999996 0.74526666]\n",
      " [0.98009929 0.98999991 1.         0.98009999]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print('Trained Q-table:')\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5622d7c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a38a69e43ac607719eac080b09ba0b62",
     "grade": false,
     "grade_id": "cell-e5b976692e3ed316",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Playing Frozen Lake\n",
    "Finally we will let the agent play Frozen Lake using the Q-table that we have learned above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447edd02",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2fc74a93a0c6315aecaaf5898cd893e9",
     "grade": false,
     "grade_id": "cell-ff4b832693eb4e4f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-warning\">\n",
    "    \n",
    "## Student Task A5.7\n",
    "\n",
    "Now we are ready to put the agent to navigate the frozen lake based on what it has learned. By simply looking at the Q-table, maybe you can already see what route the agent will be taking?<br><br>\n",
    "<strong><i><u>Your task<i></u></strong> is to run the below code and observe how the agent is navigating the frozen lake. Observe the learned Q-table to understand why the agent is taking this specific path.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3849f1a8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4650295aec707c5c0aedada2b6d1ea14",
     "grade": false,
     "grade_id": "cell-73a58f39ab224524",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_check():\n",
    "    nbgrader_exec_env = os.environ.get('NBGRADER_EXECUTION')\n",
    "    if nbgrader_exec_env is not None and nbgrader_exec_env in ['autograde', 'validate']:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Defining the run_agent function\n",
    "def run_agent(env, q_table):\n",
    "    env.reset()\n",
    "    env.render()\n",
    "    if run_check():\n",
    "        clear_output(wait=False)\n",
    "        # Watch the agent play for a few rounds using the learned Q-table\n",
    "        n_rounds = 3\n",
    "        for episode in range(n_rounds):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "\n",
    "            print('༺❁ EPISODE {} ❁༻'.format(episode+1))\n",
    "            time.sleep(1)\n",
    "\n",
    "            for step in range(max_steps_per_episode + 1):\n",
    "                clear_output(wait=True)\n",
    "                env.render()\n",
    "                time.sleep(0.3)\n",
    "\n",
    "                action = np.argmax(q_table[state, :])\n",
    "                \n",
    "                new_state, reward, done, info = env.step(action)\n",
    "\n",
    "                if done:\n",
    "                    clear_output(wait=True)\n",
    "                    env.render()\n",
    "                    if reward == 1:\n",
    "                        print('༺❁ You reached the goal after {} steps! ❁༻'.format(step + 1))\n",
    "                        time.sleep(3)\n",
    "                    elif step == max_steps_per_episode - 1:\n",
    "                        print('༺❁ No steps left ({} steps max)! ❁༻'.format(step + 1))\n",
    "                        time.sleep(3)\n",
    "                    else:\n",
    "                        print('༺❁ You fell through a hole after {} steps! ❁༻'.format(step + 1))\n",
    "                        time.sleep(3)\n",
    "                    clear_output(wait=True)\n",
    "                    break\n",
    "                elif step == max_steps_per_episode - 1:\n",
    "                    print('༺❁ No steps left ({} steps max)! ❁༻'.format(step + 1))\n",
    "                    time.sleep(3)\n",
    "                    clear_output(wait=True)\n",
    "                    break\n",
    "                state = new_state\n",
    "        time.sleep(3)\n",
    "        print('༺❁ Done ❁༻')\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "968397e4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "17488a4989d3fcbf95f4d6df0a4c636f",
     "grade": false,
     "grade_id": "cell-72d1f2d298047633",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "༺❁ Done ❁༻\n"
     ]
    }
   ],
   "source": [
    "# Run the run_agent function with the trained Q-table\n",
    "run_agent(env, q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776d9514",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7a3bbe35a15403c212afb1c1e5100bb3",
     "grade": false,
     "grade_id": "cell-ce9a443a782d5348",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## The Ice Can Get Slippery!\n",
    "![Slippery ice](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcS7iaMa5OcNHtJFKB1fZo_EvG-nQyPPTm_fsA&usqp=CAU \"Slippery ice\")\n",
    "\n",
    "So far the ice has been easy to walk on, and we have seen that the agent has not had any problem in finding an optimal path leading to the goal. Now, however, the weather has changed and the ice has suddenly become very slippery.\n",
    "In the next part of this exercise we have defined the frozen lake as:<br>\n",
    "`env_frozen = gym.make('FrozenLake-v1', is_slippery=False, map_name=\"4x4\")`<br>\n",
    "`is_slippery=True` means that there is only a 1/3 chance that the agent will move in the intended \n",
    "direction, and 2/3 chance to move in one of the perpendicular directions (1/3 and 1/3). \n",
    "E.g., if **action = left**, then:\n",
    "- P(move left) = 1/3\n",
    "- P(move up) = 1/3\n",
    "- P(move down) = 1/3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c642b7f0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "05d04fff9c229018e095ed74ead98bb7",
     "grade": false,
     "grade_id": "cell-c75890d0dc7b593e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-warning\">\n",
    "    \n",
    "## Student Task A5.8a\n",
    "\n",
    "First run the Q-Learning algorithm below and observe how the new trained Q-table looks like, and how the new statistics has become. Also observe how the behaviour of the agent has changed at run time.\n",
    "    \n",
    "## Student Task A5.8b\n",
    "Next we want you to briefly explore the impact of different values assigned to the different parameters, and try to find the most promising values that optimize the reward gained by the agent (look at the scores from the last 1000 episodes).\n",
    "The parameters in question are:\n",
    "- learning_rate ($\\alpha$)\n",
    "- discount_factor ($\\gamma$)\n",
    "- max_exploration_rate ($\\epsilon_{max}$)\n",
    "- min_exploration_rate ($\\epsilon_{min}$)\n",
    "- exploration_decay_rate ($\\zeta$)\n",
    "\n",
    "<u>Be prepared to discuss your findings with your colleges and/or teaching assistants.</u><br>\n",
    "Some additional things to think about: Why is the preferred first move of the agent to move left? Can you think of some possible approaches and strategies that could improve the performance of the agent further? Can you think of some completely different ways for how to solve this game/problem?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b36984fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training results, average reward and steps per thousand episodes:\n",
      "1000: rewards 0.036, steps 9.404\n",
      "2000: rewards 0.062, steps 15.05\n",
      "3000: rewards 0.161, steps 23.81\n",
      "4000: rewards 0.277, steps 33.19\n",
      "5000: rewards 0.357, steps 40.01\n",
      "6000: rewards 0.35, steps 44.35\n",
      "7000: rewards 0.475, steps 45.03\n",
      "8000: rewards 0.442, steps 49.46\n",
      "9000: rewards 0.425, steps 47.13\n",
      "10000: rewards 0.411, steps 46.85\n"
     ]
    }
   ],
   "source": [
    "# Initiate the FrozenLake game with slippery ice\n",
    "env_slippery = gym.make('FrozenLake-v1', is_slippery=True, map_name=\"4x4\")\n",
    "\n",
    "\n",
    "## Parameters used by the Q-training algorithm\n",
    "\n",
    "# Learning rate (value between 0 and 1)\n",
    "learning_rate = 0.9\n",
    "\n",
    "# Reward discount used in the Bellman equation (value between 0 and 1)\n",
    "discount_factor = 0.99\n",
    "\n",
    "# Upper bound for the exploration rate (value between 0 and 1)\n",
    "max_exploration_rate = 1.0\n",
    "\n",
    "# Lower bound for the exploration rate (value between 0 and 1)\n",
    "min_exploration_rate = 0.01\n",
    "\n",
    "# How much the exploration rate decays per episode (down to the min_exploration_rate, value between 0 and 1).\n",
    "exploration_decay_rate = 0.001\n",
    "\n",
    "\n",
    "# Train the agent to navigate the slippery ice\n",
    "q_table_slippery = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "if run_check():\n",
    "    q_table_slippery = train_q_learn(env_slippery, q_table_slippery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0d794daa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5488f2ffaa20a255999b5d5886c52955",
     "grade": false,
     "grade_id": "cell-a585c6db837d950e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained slippery Q-table:\n",
      "[[6.02102441e-01 1.51020564e-01 9.33706991e-02 1.48461009e-01]\n",
      " [3.44449213e-02 8.52076365e-03 1.52074997e-02 1.62132154e-01]\n",
      " [1.37940227e-02 2.76324119e-02 1.20009883e-01 1.48538845e-01]\n",
      " [3.12766359e-02 1.54643787e-03 1.25250292e-01 1.29050974e-01]\n",
      " [7.22682240e-01 6.49625501e-02 7.16509380e-02 1.98007704e-03]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [4.00809914e-04 1.17153511e-08 1.66491247e-04 1.39185040e-06]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [4.41121921e-02 5.28434078e-02 7.18899792e-02 8.83517810e-01]\n",
      " [5.68898803e-02 9.59118810e-01 1.78780760e-02 3.23872458e-02]\n",
      " [9.52214295e-01 1.16674696e-04 2.09394970e-04 1.12139712e-03]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [7.14764863e-02 7.96824036e-03 9.62178144e-01 6.49373082e-02]\n",
      " [1.77832033e-01 9.97844887e-01 1.08486215e-01 9.43707729e-02]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print('Trained slippery Q-table:')\n",
    "print(q_table_slippery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "274422be",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8bd96e962ac2c56a4355dc806c4577ab",
     "grade": false,
     "grade_id": "cell-7e7ca1a874e66864",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "༺❁ Done ❁༻\n"
     ]
    }
   ],
   "source": [
    "# Run the agent to see how the agent now navigates the slippery ice.\n",
    "run_agent(env_slippery, q_table_slippery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5b0aac-a50d-4d12-941e-de64ca9838f4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "95112e0172b23d920d17b0e9162394d4",
     "grade": false,
     "grade_id": "cell-b1cd39a5c2e1aa0c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa3f9f6f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "75e22a95508c2609d04eb040c2c27f99",
     "grade": false,
     "grade_id": "cell-b2698db523d0b46d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Language Models\n",
    "<img src=\"https://www.wisecube.ai/wp-content/uploads/2023/05/Featured-Blog-Image-A-Comprehensive-Overview-of-Large-Language-Models-1080x675.jpg\" alt=\"drawing\" width=\"800\"/>\n",
    "\n",
    "Language models, or large language models (LLMs), primarily focuses on classifying and generating text (next token prediction), as well as generating condensed vector representations of texts, typically referred to as embeddings. \n",
    "Pre-training on large datasets, in conjunction with task-specific supervised fine-tuning, demonstrates state of the art performance in many natural language processing (NLP) tasks, including few-shot and zero-shot scenarios. In this assignment we will explore the application of LLM in completing two tasks.\n",
    "\n",
    "The transformer library by Huggingface will be used for ease of implementation. It is built for simplifying the implementation of NLP applications. Huggingface is also known for their model and dataset hub, where they host various trained models and datasets which allows users to easily exchange trained models and datasets, as well as showcase demos of their models.\n",
    "\n",
    "\n",
    "### Learning Goals\n",
    "After successfully completing this part of the assignment, you should be able to:\n",
    "- solve NLP tasks using existing LLM in a zero-shot manner.\n",
    "- learn how to use existing LLMs to generate texts.\n",
    "- understand the basics of how to evaluate generative LLMs.\n",
    "- understand how to use huggingface library and Huggingface model hub.\n",
    "- implement similarity search for doing query-based document retrieval.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Language Generation](#language_gneration)\n",
    "2. [Zero-Shot Information Retrieval](#zero_shot_information_retrieval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c40a687-49b2-4616-a714-2e026e5a2e78",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "62eaa8bcfa6e472afb578cd5fc1e01ec",
     "grade": false,
     "grade_id": "cell-d918363a6554142c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Language Generation <a class=\"anchor\" id=\"language_gneration\"></a>\n",
    "\n",
    "The task involves generating textual content in response to a given prompt. \n",
    "It serves as the foundation for numerous natural language generation (NLG) tasks. \n",
    "In this work, the GPT-Neo Model with 68 Million parameters will be utilized. \n",
    "Despite its very modest size, the model has been pre-trained on the TinyStories dataset, enabling it to generate plausible sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1b837fd7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "10356f2eecd0b7d2d13da5e076099854",
     "grade": false,
     "grade_id": "cell-46515bcaf2e6c918",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e5aec1f66704b0fa5777cc675f9c976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/968 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "999822881f224a30a48541a210f4dfbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/291M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2df84e6a130e426eb9d3843b5d4a4e1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/722 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78c464b3f3c6467cb41e67fd64795657",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ea092e4fb97411993cb74a5851ee8b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78ce38e2eae042c0afdf26a6ea2b27e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e15b31e452f84c87bdaaafb38d4ab2fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/438 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the prompt: \"Hi, How are you?\"\n",
      "The generated text corresponding to this prompt is: \n",
      "\n",
      " Hi, How are you? Do you want to play with me?\"\n",
      "\n",
      "The boy looked at her and said, \"No, I don't. Go away. You are too small and silly. You don't know how to play. You are a baby.\"\n",
      "\n",
      "Lily felt sad and angry. She said, \"No, you are not. You are mean and rude. You don't know how to be nice. You don't know how to have fun. You don't know how to be a friend.\"\n",
      "\n",
      "The boy laughed and said, \"You are just a baby. You don't know how to have fun. You don't know how to be happy. You don't know how to be a friend. You are a bully.\"\n",
      "\n",
      "Lily felt hurt and scared. She said, \"No, I am not. I am a friend. I am a bully. I don't know how to be nice. I don't know how to be \n",
      "\n",
      "\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# Import the libraries we will be using\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer,logging\n",
    "logging.set_verbosity(logging.ERROR )\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Helper function for grading\n",
    "def run_check():\n",
    "    nbgrader_exec_env = os.environ.get('NBGRADER_EXECUTION')\n",
    "    if nbgrader_exec_env is not None and nbgrader_exec_env in ['autograde', 'validate']:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Model checkpoint name from Huggingface hub\n",
    "model_name = 'roneneldan/TinyStories-33M'\n",
    "\n",
    "# Load the pre-trained model\n",
    "# AutoModelForCausalLM is for language model tasks\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Load the tokenizer associated with the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Example prompt for text generation\n",
    "prompt = \"Hi, How are you?\"\n",
    "\n",
    "# Tokenize the prompt\n",
    "# return_tensors=\"pt\" specifies PyTorch tensors (use \"tf\" for TensorFlow)\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generates up to max_length words from the prompt.\n",
    "# top_p, num_beams, etc., control generation (details: https://huggingface.co/transformers/v4.2.2/_modules/transformers/generation_utils.html).\n",
    "# return_dict_in_generate and output_scores: return prediction scores along with tokens.\n",
    "# pad_token_id:ID of the padding token.\n",
    "outputs = model.generate(input_ids, max_length=200, return_dict_in_generate=True, output_scores=True, pad_token_id=50256)\n",
    "\n",
    "# Decode the generated tokens back to text\n",
    "# skip_special_tokens=True: special tokens such as [CLS], [SEP] which we skip in the output text.\n",
    "output_text = tokenizer.decode(*outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f'Given the prompt: \"{prompt}\"\\nThe generated text corresponding to this prompt is: \\n\\n {output_text} \\n\\n\\n ')\n",
    "\n",
    "# outputs[1] contains the logit scores for each token. \n",
    "# Typically, these logits are converted to probabilities using the Softmax function.\n",
    "probabilities = [max(torch.nn.functional.softmax(x)[0].tolist()) for x in outputs[1]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97adc70d-14c3-4021-918b-2f86dd93761a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5d58bbf6057711531554c31b8e522f68",
     "grade": false,
     "grade_id": "cell-fd9e04be88465922",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-warning\">\n",
    "    \n",
    "## Student Task A5.9\n",
    "\n",
    "Your task is to implement the `text_generation()` function, whose stages are already demonstrated in the cell above. The function will be invoked for three prompts with respective max_len values of 50, 100, and 60.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b296bc50-765f-4a6a-8643-7bb383d8b54f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "119934907ce17f886aa181578497568e",
     "grade": false,
     "grade_id": "cell-77b421e67d1de8b9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "once upon a time  there was a little girl named Lucy. She was three years old and loved to explore. One day, Lucy was walking in the park when she saw something shiny. She went closer and saw it was a big, round,\n"
     ]
    }
   ],
   "source": [
    "# Implementing Text Generation\n",
    "def text_generation(model,tokenizer,prompt,max_len):\n",
    "    ''' \n",
    "    Given a prompt and model, generate text of length max_len.\n",
    "    \n",
    "    Args:\n",
    "    model: The pre-trained language model\n",
    "    tokenizer: The tokenizer associated with the model\n",
    "    prompt (str): The input text to generate from\n",
    "    max_len (int): The maximum length of the generated text\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (generated_text, token_probabilities)\n",
    "    \n",
    "    ''' \n",
    "    \n",
    "    # input_ids = tokenizer.\n",
    "    # outputs = model.generate()\n",
    "    # output_text = \n",
    "    # probabilities = \n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors =\"pt\")\n",
    "    outputs = model.generate(input_ids, max_length=max_len, return_dict_in_generate=True, output_scores=True)\n",
    "    output_text =tokenizer.decode(*outputs[0], skip_special_tokens=True)\n",
    "    probabilities = [max(torch.nn.functional.softmax(x)[0].tolist()) for x in outputs[1]]\n",
    "    \n",
    "    return output_text,probabilities\n",
    "\n",
    "prompt = \"once upon a time \"\n",
    "text,prob = text_generation(model,tokenizer,prompt,max_len=50)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4c0c3ab5-9ae0-464a-babd-b7bd47be0cc6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "be9d682a422308ef849151a08ffb9ddc",
     "grade": true,
     "grade_id": "cell-973c5451c1c3107d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this cell is for tests\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daa1300",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1cedc1ef6a7571bf459dad0316edcdac",
     "grade": false,
     "grade_id": "cell-69a1a51b8558eb6c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Prompt Engineering\n",
    "Large language models behave differently based on the prompt, resulting in a new discipline called prompt engineering that focuses on the optimal design of prompts for maximum output. Let's see if we can find some engaging prompts to generate meaningful text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "09127202",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a joke for me \n",
      "\n",
      "The little girl thought for a moment and then said, \"Why did the chicken cross the road?\"\n",
      "\n",
      "The farmer laughed and said, \"I don't know, why?\"\n",
      "\n",
      "The little girl smiled and said, \"Because it wanted to get to the other side!\"\n",
      "\n",
      "The farmer was so happy that he gave the little girl a big hug. From then on, the farmer and the little girl were good friends.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## ==========================================================\n",
    "## Play with prompts, try it yourself\n",
    "prompt = 'Write a joke for me '\n",
    "max_length = 100\n",
    "## ==========================================================\n",
    "\n",
    "if run_check():\n",
    "    text, _ = text_generation(model,tokenizer,prompt,max_len=max_length)\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f2fcfb-4700-464e-ac43-6291af501de2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "372c4c3b8531e73a3058820d271970d1",
     "grade": false,
     "grade_id": "cell-b2398fe34541aa9e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Evaluation of Language Models\n",
    "\n",
    "Evaluating Natural Language Processing (NLP) models, particularly Large Language Models (LLMs), is essential for understanding their capabilities and limitations. There are two primary approaches to evaluation:\n",
    "\n",
    "#### 1. Extrinsic Evaluation\n",
    "This method assesses a model's performance on specific, downstream tasks, providing a practical measure of the model's effectiveness in applied scenarios.\n",
    "\n",
    "#### 2. Intrinsic Evaluation \n",
    "This approach involves identifying inherent properties of a model to estimate its quality, independent of any specific task it was designed to perform. These properties often relate to the model's underlying linguistic capabilities.\n",
    "\n",
    "---\n",
    "\n",
    "### Perplexity: A Key Intrinsic Metric\n",
    "\n",
    "One critical metric for intrinsic evaluation is **perplexity**, which quantifies the uncertainty a language model exhibits during word generation. It reflects how well the model can predict a sequence of text.\n",
    "\n",
    "#### Calculation of Perplexity\n",
    "\n",
    "Perplexity is calculated as follows:\n",
    "\n",
    "1. For each word $x_t$ in the corpus, compute its probability based on all preceding words.\n",
    "2. Take the inverse of this probability.\n",
    "3. Calculate the product of these inverse probabilities for all words.\n",
    "4. Normalize this product by raising it to the power of $\\frac{1}{N}$, where $N$ is the total number of words.\n",
    "\n",
    "Mathematically, this is expressed as:\n",
    "\n",
    "$$\n",
    "\\text{Perplexity} = \\sqrt[N]{\\prod_{t=1}^N \\frac{1}{P_{LM}(x_t|x_1, \\ldots, x_{t-1})}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $N$ is the total number of words in the corpus.\n",
    "- $P_{LM}(x_t|x_1, \\ldots, x_{t-1})$ is the conditional probability of the $t$-th word given all previous words.\n",
    "\n",
    "> **Note:** To ensure numerical stability and avoid exceptions when the probability $P_{LM}(x_t|x_1, \\ldots, x_{t-1})$ is zero, a small epsilon value can be used.\n",
    "\n",
    "#### Interpretation of Perplexity\n",
    "\n",
    "A lower perplexity value indicates better performance, as it suggests the model is less uncertain in predicting words.\n",
    "\n",
    "---\n",
    "\n",
    "#### Limitations and Considerations\n",
    "\n",
    "While perplexity is a valuable metric, it  may not always correlate directly with performance on specific downstream tasks. Moreover Perplexity scores are not directly comparable between models with different vocabularies or tokenization schemes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6c647b-4923-4000-9eb3-4c0fba2035c9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "262298bf75f5cc3374973451728fba41",
     "grade": false,
     "grade_id": "cell-a2a1ed9eb6dc129b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-warning\">\n",
    "    \n",
    "## Student Task A5.10\n",
    "\n",
    "Your task is to implement the function `calculate_perplexity()`, which computes the perplexity based on the output probabilities of the models. \n",
    "    \n",
    "Note: Element t in the input list probabilities is $P_{LM}(x_{t}|x_{1},x_{2}...,x_{t-1})$. and $N=$ len(probabilities)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "eae346e1-997f-45ae-88e7-02ef52ee137e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5dfd4ce1ffb91f6ec99e02ddeca7e90b",
     "grade": false,
     "grade_id": "cell-6e20fb6c4e76c72c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 536.9704618928898\n"
     ]
    }
   ],
   "source": [
    "# Implementing perplexity calculation\n",
    "def calculate_perplexity(probabilities,epsilon=1e-12):\n",
    "    \"\"\"\n",
    "    Calculate perplexity given a list of probabilities.\n",
    "    \n",
    "    Args:\n",
    "    probabilities (list): List of probabilities for each token.\n",
    "    epsilon (float, optional): A small value to prevent division by zero or log of zero.\n",
    "\n",
    "    \n",
    "    Returns:\n",
    "    float: Perplexity value.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "    perplexity = 1\n",
    "    for i in range(len(probabilities)):\n",
    "        if probabilities[i] != 0:            \n",
    "            perplexity = perplexity * (1 / probabilities[i])\n",
    "        else:\n",
    "            perplexity = perplexity * (1 / epsilon)\n",
    "    return perplexity ** (1 /len(probabilities))\n",
    "\n",
    "# # Unit Test example:\n",
    "probs = [0.8, 0.7, 0.1,0.4,0.0]\n",
    "perplexity = calculate_perplexity(probs)\n",
    "print(f\"Perplexity: {perplexity}\") # Output should be Perplexity: 536.970462"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479c04e1-2b64-41e2-bc6e-d388e7d338e3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "95e96b34b8ef934657e9e8662b3484e0",
     "grade": true,
     "grade_id": "cell-87af2727f38ded3c",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this cell is for tests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b35079-5b21-4000-a6cd-d7897dab6f8a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ed8dcd313138d61b06402003f7bfd41f",
     "grade": false,
     "grade_id": "cell-be3175657f029242",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Zero-Shot Information Retrieval <a class=\"anchor\" id=\"zero_shot_information_retrieval\"></a>\n",
    "\n",
    "Information Retrieval aims to find and extract information relevant to a user's query. It's widely used in web search, question-answering systems, personal assistants, and chatbots.\n",
    "In this task, we'll use a pre-trained BERT model (110 million parameters) for zero-shot information retrieval.\n",
    "\n",
    ">**Note:** \"Zero-shot\" means the model hasn't been specifically trained for `information retrieval` on this `dataset`.\n",
    "---\n",
    "\n",
    "#### Task Overview: \n",
    "You are provided with 1,000 short stories and their unique identifiers in `story.csv`. The objective is to retrieve the most relevant story for each query, which consists of 1-2 sentences. The `query.csv` file contains 100 such queries. Your goal is to determine and return the unique identifier of the most relevant story for each query.\n",
    "\n",
    "\n",
    "#### Solution Strategy: \n",
    "We will leverage a language model to generate embeddings—vectorized representations—of both the stories and queries. By calculating the cosine similarity between these embeddings, we will identify and retrieve the story that best matches each query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5d84b0c0-c7fa-4ac5-bb9f-0beff5f2d4c7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "364e25f045f111d3b20daaa9ac50bd58",
     "grade": false,
     "grade_id": "cell-6a32a238dbd4be40",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>story_uid</th>\n",
       "      <th>story</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>774</td>\n",
       "      <td>Once upon a time, there was a little car name...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90</td>\n",
       "      <td>Once upon a time, there was a white bunny. Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>961</td>\n",
       "      <td>Once upon a time, there was a little girl nam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>255</td>\n",
       "      <td>Once upon a time, there was a little girl who...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>382</td>\n",
       "      <td>One day, a mommy and her little girl went to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   story_uid                                              story\n",
       "0        774   Once upon a time, there was a little car name...\n",
       "1         90   Once upon a time, there was a white bunny. Th...\n",
       "2        961   Once upon a time, there was a little girl nam...\n",
       "3        255   Once upon a time, there was a little girl who...\n",
       "4        382    One day, a mommy and her little girl went to..."
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the libraries we will be using\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "\n",
    "# 1000 stories with their unique id (story_uid) which is our information database \n",
    "story_df = pd.read_csv('story.csv')\n",
    "story_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b3862a94",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "99fca9490fa0296ed4efc264983422ae",
     "grade": false,
     "grade_id": "cell-199f9cf79c9bbf1e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' \"Give that back!\" Beth said. Her brother ignored her and tried to run away with it, but Beth was determined not to let him take it',\n",
       " '\" From that day on, Lily played with her toy ambulance and pretended to be a smart doctor who helped people in need. She knew that one day, she would be just like the real ambulance and help even more people']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We are given query.csv file which contains 100 queries.\n",
    "# We need to search and return the most similar story (from story_df) corresponding to each query.\n",
    "query_df = pd.read_csv('query.csv')\n",
    "query_list = list(query_df['query'])\n",
    "query_list[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfac0c5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aa7f2c294afd3a00268a62b2416e0675",
     "grade": false,
     "grade_id": "cell-270de8721c16acea",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Extracting Embedding Vectors Using a Pre-Trained LLM Model\n",
    "\n",
    "There are various methods to extract embeddings from a pre-trained language model, each suited for different use cases. The output from the last layer of the language model typically has the shape `(B, T, d)`, where:\n",
    "- `B` is the batch size,\n",
    "- `T` is the sequence length (number of tokens), and\n",
    "- `d` is the feature dimension in each token's representation.\n",
    "\n",
    "Assuming `output_tensor` (a NumPy/ PyTorch tensor) is the output from the last layer with shape `(B, T, d)`, where `B=1`, the following are some common approaches to extract embeddings:\n",
    "\n",
    "1. **Last Token Embedding:** Extracts the embedding of the last token in the sequence: `last_token_embedding = output_tensor.squeeze(0)[-1, :]`.\n",
    "2. **[CLS] Token Embedding:** Uses the embedding of the [CLS] token, which is often located at the first position and represents the entire sequence (specific to BERT models): `cls_token_embedding = output_tensor.squeeze(0)[0, :]`\n",
    "3. **Mean Pooling:** Computes the mean of all token embeddings, providing a generalized representation of the sequence: `mean_embedding = output_tensor.mean(dim=1).squeeze(0)`\n",
    "4. **Weighted Mean Pooling:** Applies a weighted mean where the influence of each token's embedding decreases linearly from the last token to the first. This can be implemented using the `get_weighted_mean_emb()` function : `weighted_mean_embedding = get_weighted_mean_emb(last_layer_embs)`\n",
    "\n",
    "For our model, we found that Mean Pooling and especially Weighted Mean Pooling yield the best results in terms of capturing the sequence's semantic information.\n",
    "\n",
    "> **Note:** For models of the `AutoModelForCausalLM` type, the `model.transformer()` method is used to obtain the output from the last feature layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b78f49ef",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "213053b5e9f3d7ad42356eaefbba57a1",
     "grade": false,
     "grade_id": "cell-b6e929797ed94257",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Function to extract mean weighted embeddings.\n",
    "def get_weighted_mean_emb(last_layer_embs):\n",
    "    phrase_embedding_mean_weighted = torch.zeros(last_layer_embs.size(-1))\n",
    "    len_s = last_layer_embs.size(0)\n",
    "    for j, j_token in enumerate(last_layer_embs):\n",
    "        phrase_embedding_mean_weighted += ((j / len_s) * j_token)\n",
    "    normalized = F.normalize(phrase_embedding_mean_weighted, p=2, dim=-1)\n",
    "    return normalized.detach().numpy()\n",
    "\n",
    "\n",
    "# Here we have precomputed the embeddings for each story to save some time.\n",
    "load_precomputed_embeddings = True\n",
    "\n",
    "if not load_precomputed_embeddings and run_check():\n",
    "    story_emb_list = []\n",
    "    for story in tqdm.tqdm(story_df['story']):\n",
    "        # Mean\n",
    "        #story_emb_list.append(model.transformer(tokenizer.encode(story, return_tensors=\"pt\"))[0].mean(dim=1).squeeze(0).detach().numpy())\n",
    "        \n",
    "        # Weighted mean, using the get_weighted_mean_emb() function\n",
    "        story_emb_list.append(get_weighted_mean_emb(model.transformer(tokenizer.encode(story, return_tensors=\"pt\"))[0].squeeze(0)))\n",
    "        \n",
    "    story_emb_array = np.stack(story_emb_list)\n",
    "else:\n",
    "    story_emb_array = np.load('story_embeddings.npy')\n",
    "\n",
    "uids = np.array(list(story_df['story_uid']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "11d4eccf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "afabfa95982ceefe8b1d45b305e0c7a5",
     "grade": false,
     "grade_id": "cell-5b2dccfe1447d239",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We also want to get the embeddings of each query.\n",
    "query_emb_list = []\n",
    "for query in query_list:\n",
    "    query_emb_list.append(get_weighted_mean_emb(model.transformer(tokenizer.encode(query, return_tensors=\"pt\"))[0].squeeze(0)))\n",
    "query_emb_array = np.stack(query_emb_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097c2c66",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "09701431cabf549a526ba2b6fb6aa4ca",
     "grade": false,
     "grade_id": "cell-ee9990b0db67d80c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Visualization of Embeddings\n",
    "\n",
    "Each stories and queries are embedded to a 768 dimensional vector space where their position and distance represents their semantic similarities. For visualization purposes, those vectors are projected down to 2 dimension using tSNE method (PCA is also an alternatives) as shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "53e8d729",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f0461b57188f8acf1593890c7c4ba92a",
     "grade": false,
     "grade_id": "cell-364a78e3b322fddd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## The code to generate TSNE plot\n",
    "#from sklearn.manifold import TSNE\n",
    "#import matplotlib.pyplot as plt\n",
    "#story_embedded = TSNE(n_components=2, init='random', perplexity=3).fit_transform(story_emb)\n",
    "#plt.scatter(story_embedded[:,0],story_embedded[:,1],marker='o')\n",
    "#query_embedded = TSNE(n_components=2, init='random', perplexity=3).fit_transform(query_emb[:30])\n",
    "#plt.scatter(query_embedded[:,0],query_embedded[:,1],marker='+')\n",
    "#plt.legend([\"story embeddings\" , \"Query embeddings\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1179fe64-42d2-4b6e-b46a-08a95d023d14",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5bc0de414df534e1d30d4ad8080c1d05",
     "grade": false,
     "grade_id": "cell-d0535b17a2a2dcf1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<img src=\"tsne.png\" alt=\"drawing\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f05cfae",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a5b96d96535cd295eef5a9b437a2b2d1",
     "grade": false,
     "grade_id": "cell-df8f2f7c66d7c7d3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The information retrieval problem has been simplified to a search problem in a vector space where both the data and the query reside. To determine the closest data point for each query, we will utilize the cosine similarity measure.\n",
    "\n",
    "### Cosine Similarity\n",
    "\n",
    "Cosine similarity is a measure of similarity between two non-zero vectors in an n-dimensional space. It quantifies the cosine of the angle between the vectors, providing an indication of their orientation rather than their magnitude. This metric is widely used to assess the similarity between documents or feature vectors.\n",
    "\n",
    "Given two vectors of attributes $\\mathbf{a}$ and $\\mathbf{b}$, the cosine similarity $\\cos(\\theta)$ is defined as:\n",
    "\n",
    "$$\n",
    "\\cos(\\theta) = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{\\|\\mathbf{a}\\| \\|\\mathbf{b}\\|}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{a} \\cdot \\mathbf{b}$ represents the dot product of the vectors.\n",
    "- $\\|\\mathbf{a}\\|$ and $\\|\\mathbf{b}\\| $ are the magnitudes (or Euclidean norms) of the vectors $\\mathbf{a}$ and $ \\mathbf{b}$, respectively.\n",
    "\n",
    "The magnitude of a vector $\\mathbf{a}$ is calculated as:\n",
    "\n",
    "$$\n",
    "\\|\\mathbf{a}\\| = \\sqrt{a_1^2 + a_2^2 + \\cdots + a_n^2}\n",
    "$$\n",
    "\n",
    "\n",
    "Cosine similarity ranges from -1 to 1:\n",
    "- A value of 1 indicates that the vectors are identical in direction.\n",
    "- A value of 0 indicates that the vectors are orthogonal (i.e., completely dissimilar in terms of direction).\n",
    "- A value of -1 indicates that the vectors are diametrically opposed.\n",
    "\n",
    "> Note: In Python, the magnitude of a vector can be computed using the `np.linalg.norm(vector)` function from the NumPy library.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8279e78",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "93dc659447cf87052013586be0ae4dec",
     "grade": false,
     "grade_id": "cell-445d6a6d28372419",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-warning\">\n",
    "    \n",
    "## Student Task A5.11\n",
    "\n",
    "Your task is to implement the function `cosine_similarity()`, which computes the similarity between two vectors vector1 and vector2. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a2b0d897",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f19c9517a7f1fbf01c4604e8fae8d16a",
     "grade": false,
     "grade_id": "cell-36bc8c27e065585e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4787905921770076\n"
     ]
    }
   ],
   "source": [
    "# implement cosine similarity measure between two vectors.\n",
    "def cosine_similarity(vector1, vector2):\n",
    "    '''\n",
    "    Calculate Cosine similarity between two vectors.\n",
    "    \n",
    "    Parameters:\n",
    "    vector1: numpy array of shape (N,)\n",
    "    vector2: numpy array of shape (N,)\n",
    "    \n",
    "    Returns:\n",
    "    similarity: cosine similarity value.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "    norm1 = np.linalg.norm(vector1)\n",
    "    norm2 = np.linalg.norm(vector2)\n",
    "    similarity = np.dot(vector1, vector2) / (norm1 * norm2)\n",
    "    \n",
    "    \n",
    "    return similarity\n",
    "\n",
    "\n",
    "# Unit test case\n",
    "a = np.array([1.0, 4.1, 5.6])\n",
    "b = np.array([0.1, -1.5, 3.2])\n",
    "sim_score = cosine_similarity(a, b)  # sim_score = 0.47879\n",
    "print(sim_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e4405bf4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dc93fd0099a640bbc57bb1411de842a4",
     "grade": true,
     "grade_id": "cell-e9f5e28fbc895895",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this cell is for tests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1218d1b3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c7079193395db3232831802945241d51",
     "grade": false,
     "grade_id": "cell-32eb0964b5665dce",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Example of document retrieval for a single query\n",
    "\n",
    "Feel free to change the `query` string in the below cell and see what story is retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4197d4aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: \"the dog\"\n",
      "\n",
      "Retrieved story:   Once upon a time, there was a rare puppy. The puppy was very cute. One day, the puppy fell asleep in the sun. The sun was nice and warm. It felt so good that the puppy started to change. His fur grew brighter and brighter. \"Wow!\" said a girl. She was walking by. \"He looks so different. He was grey before. Now he is white and fluffy!\" The puppy opened his eyes and said, \"I changed!\" He jumped up and started to bark. The girl laughed. \"You sure did change! You are the most rare and beautiful puppy I have ever seen.\" The puppy smiled and started to lick the girl's face. He was so happy that he had changed. \n",
      "\n",
      "Unique id of retreived document: 737\n"
     ]
    }
   ],
   "source": [
    "## ==========================================================\n",
    "## A single query. Feel free to try your own query!\n",
    "#query = 'picking berries'\n",
    "#query = 'loud sound'\n",
    "query = 'the dog'\n",
    "## ==========================================================\n",
    "\n",
    "if run_check():\n",
    "    print(f'Query: \"{query}\"')\n",
    "    \n",
    "    # Embedding of the query\n",
    "    #query_embed = model.transformer(tokenizer.encode(query, return_tensors=\"pt\"))[0].mean(dim=1).squeeze(0).detach().numpy()\n",
    "    #query_embed = model.transformer(tokenizer.encode(query, return_tensors=\"pt\"))[0].squeeze(0)[-1].detach().numpy()\n",
    "    query_emb = get_weighted_mean_emb(model.transformer(tokenizer.encode(query, return_tensors=\"pt\"))[0].squeeze(0))\n",
    "\n",
    "    # Cosine similarity of the query with all the documnts/stories in our data\n",
    "    similarity_list = [cosine_similarity(query_emb, x) for x in story_emb_array]\n",
    "\n",
    "    # Index of the retrived document and its similarity score to the query.\n",
    "    index = similarity_list.index(max(similarity_list))\n",
    "    score = similarity_list[index]\n",
    "    story = story_df.iloc[index][\"story\"]\n",
    "    uid = story_df.iloc[index]['story_uid']\n",
    "    \n",
    "    print(f'\\nRetrieved story: {story_df.iloc[index][\"story\"]}')\n",
    "    print(f'\\nUnique id of retreived document: {uid}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e574ee",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "49c57970b632cad6ef832c5e0c7893fe",
     "grade": false,
     "grade_id": "cell-49ab71d9ae4660d0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-warning\">\n",
    "    \n",
    "## Student Task A5.12\n",
    "\n",
    "Your task is to implement the function `predict_uid()`, which yields the predicted document uids (predicted_uid_list). The instructions for a single example are already shown in the cell above.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "70dc492a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1e84e52e310aad7078753100902b6d19",
     "grade": false,
     "grade_id": "cell-d7e99edb6dbd4d97",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[957, 245]\n"
     ]
    }
   ],
   "source": [
    "def predict_uid(query_embs, story_embs, story_df):\n",
    "    '''\n",
    "    Predict the uid for each query in the query_emb array.\n",
    "    \n",
    "    Args:\n",
    "        query_emb (np.array): Array of query embeddings.\n",
    "        story_emb (np.array): Array of story embeddings.\n",
    "        story_df (pd.DataFrame): DataFrame containing story information including 'story_uid'.\n",
    "    Return:\n",
    "        predicted_uid_list (List): List of predicted story uids corresponding to the queries.\n",
    "    \n",
    "    '''\n",
    "    #list to append predicted story/document uids corresponding to the queries.  \n",
    "    predicted_uid_list = []\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "    for query_emb in query_embs:\n",
    "        similarity_list = [cosine_similarity(query_emb, x) for x in story_embs]\n",
    "        index = similarity_list.index(max(similarity_list))\n",
    "        uid = story_df.iloc[index]['story_uid']\n",
    "        predicted_uid_list.append(uid)\n",
    "        \n",
    "    return predicted_uid_list\n",
    "\n",
    "# #unit test example:\n",
    "predicted_uids_test = predict_uid(query_emb_array[:2],story_emb_array,story_df) # correct output should be predicted_uids_test = [957, 245]\n",
    "print(predicted_uids_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "08811c87",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "09186e432dd97b70823cd8cbc9125b46",
     "grade": true,
     "grade_id": "cell-4722a2f16e1305ef",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this cell is for tests\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b2adb6-0100-4f4e-b7da-c406961fc181",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1805eb-8719-4817-be17-c4fdb5458a74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
